{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXwMDlcYE3-d",
        "outputId": "e4c33efb-3180-4bad-d228-70f080635486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Data Collection ---\n",
            "Loaded: /content/drive/MyDrive/DS With Generative AI/Machine Learning/coin_gecko_2022-03-16.csv\n",
            "Loaded: /content/drive/MyDrive/DS With Generative AI/Machine Learning/coin_gecko_2022-03-17.csv\n",
            "Total merged data points: 1000\n",
            "\n",
            "--- 2. Data Preprocessing ---\n",
            "Missing values before imputation: 29\n",
            "Missing values imputed using median.\n",
            "\n",
            "--- 3. Feature Engineering ---\n",
            "Target 'Liquidity_Index' and new features created.\n",
            "Data Split: Train=800, Test=200\n",
            "\n",
            "--- 4. Model Training & Evaluation ---\n",
            "RandomForestRegressor Model Trained.\n",
            "\n",
            "Evaluation Metrics:\n",
            "  Root Mean Square Error (RMSE): 40.0011\n",
            "  Mean Absolute Error (MAE): 10.7598\n",
            "  R² Score: 0.3146\n",
            "\n",
            "Model and scaler successfully saved as 'liquidity_predictor.pkl' and 'data_scaler.pkl'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import joblib\n",
        "\n",
        "FILE_PATHS = [\n",
        "    '/content/drive/MyDrive/DS With Generative AI/Machine Learning/coin_gecko_2022-03-16.csv',\n",
        "    '/content/drive/MyDrive/DS With Generative AI/Machine Learning/coin_gecko_2022-03-17.csv'\n",
        "]\n",
        "TARGET_COLUMN = 'Liquidity_Index'\n",
        "MODEL_FILENAME = 'liquidity_predictor.pkl'\n",
        "\n",
        "def load_and_merge_data(file_paths):\n",
        "\n",
        "    print(\"--- 1. Data Collection ---\")\n",
        "    data_frames = []\n",
        "    for file in file_paths:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            data_frames.append(df)\n",
        "            print(f\"Loaded: {file}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"ERROR: File not found: {file}\")\n",
        "\n",
        "    if not data_frames:\n",
        "        raise ValueError(\"No data files were loaded successfully.\")\n",
        "\n",
        "    df_combined = pd.concat(data_frames, ignore_index=True)\n",
        "    df_combined.sort_values(by=['coin', 'date'], inplace=True)\n",
        "    print(f\"Total merged data points: {len(df_combined)}\")\n",
        "    return df_combined\n",
        "\n",
        "def preprocess_data(df):\n",
        "    print(\"\\n--- 2. Data Preprocessing ---\")\n",
        "\n",
        "    df.drop(columns=['coin', 'symbol', 'date'], inplace=True, errors='ignore')\n",
        "\n",
        "    print(f\"Missing values before imputation: {df.isnull().sum().sum()}\")\n",
        "    df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "    print(\"Missing values imputed using median.\")\n",
        "\n",
        "    # Ensure all columns are numeric after dropping non-numeric ones\n",
        "    df = df.select_dtypes(include=np.number)\n",
        "\n",
        "    return df\n",
        "\n",
        "def feature_engineering(df):\n",
        "\n",
        "    print(\"\\n--- 3. Feature Engineering ---\")\n",
        "    df['Liquidity_Ratio'] = df['24h_volume'] / (df['mkt_cap'] + 1e-6)\n",
        "\n",
        "    df[TARGET_COLUMN] = df['Liquidity_Ratio'] / (np.abs(df['24h']) + 0.001)\n",
        "\n",
        "    df['Vol_Price_Ratio'] = df['24h_volume'] / (df['price'] + 1e-6)\n",
        "\n",
        "    df.drop(columns=['Liquidity_Ratio'], inplace=True)\n",
        "\n",
        "    print(f\"Target '{TARGET_COLUMN}' and new features created.\")\n",
        "    return df\n",
        "\n",
        "def prepare_for_training(df):\n",
        "\n",
        "    X = df.drop(columns=[TARGET_COLUMN])\n",
        "    y = df[TARGET_COLUMN]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled, y, test_size=0.2, shuffle=False, random_state=42\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, scaler\n",
        "\n",
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test):\n",
        "\n",
        "    print(\"\\n--- 4. Model Training & Evaluation ---\")\n",
        "\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        max_depth=10\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"RandomForestRegressor Model Trained.\")\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "\n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    print(f\"  Root Mean Square Error (RMSE): {rmse:.4f}\")\n",
        "    print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"  R² Score: {r2:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "\n",
        "        df_raw = load_and_merge_data(FILE_PATHS)\n",
        "\n",
        "        df_processed = preprocess_data(df_raw.copy())\n",
        "\n",
        "        df_featured = feature_engineering(df_processed.copy())\n",
        "\n",
        "        X_train, X_test, y_train, y_test, scaler = prepare_for_training(df_featured)\n",
        "        print(f\"Data Split: Train={len(X_train)}, Test={len(X_test)}\")\n",
        "\n",
        "        model = train_and_evaluate_model(X_train, X_test, y_train, y_test)\n",
        "\n",
        "        joblib.dump(model, MODEL_FILENAME)\n",
        "        joblib.dump(scaler, 'data_scaler.pkl')\n",
        "        print(f\"\\nModel and scaler successfully saved as '{MODEL_FILENAME}' and 'data_scaler.pkl'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during the pipeline execution: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LXU-B2zkFJOS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}